{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3TftJl5snU3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import copy\n",
    "import math\n",
    "import operator\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0LoB3i8d_6s"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"cains.txt\"):\n",
    "    cains_fp = \"cains.txt\"\n",
    "else:\n",
    "    cains_fp = \"/content/cains.txt\"\n",
    "with open(cains_fp, 'r', encoding=\"utf-8\") as cains_file:\n",
    "    text = cains_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysing cosine similarity between pages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWGph1ERzE4z"
   },
   "outputs": [],
   "source": [
    "def get_document_word_vector(text):\n",
    "    print('input text length: ', len(text))\n",
    "    print('input text beginning: ', text[:200])\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_no_punct = text.translate(translator)\n",
    "    print('(no punctuation) text length: ', len(text_no_punct))\n",
    "    print('(no punctuation) text beginning: ', text_no_punct[:200])\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text_no_punct.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    print('total tokens: ', len(token_counts))\n",
    "    print('sample tokens: ', tokens[:100])\n",
    "    print('tokens sorted by occurences: ', token_counts.most_common()[:100])\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [x for x in tokens if x not in stopwords]\n",
    "    token_counts = Counter(tokens)\n",
    "    print('(no stopwords) total tokens: ', len(token_counts))\n",
    "    print('(no stopwords) sample tokens: ', tokens[:100])\n",
    "    print('(no stopwords) tokens sorted by occurences: ', token_counts.most_common()[:100])\n",
    "\n",
    "    document_vector = {}\n",
    "    tokens_num = len(tokens)\n",
    "    for key, value in token_counts.most_common():\n",
    "        document_vector[key] = value / tokens_num\n",
    "    print('built word vector of length', len(document_vector))\n",
    "    lexicon = sorted(set(tokens))\n",
    "    return document_vector, lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_book_wv, all_book_lexicon = get_document_word_vector(text)\n",
    "print(all_book_lexicon[:50])\n",
    "print(all_book_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vector = OrderedDict((token, 0) for token in all_book_lexicon)\n",
    "print(len(zero_vector))\n",
    "#print(zero_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLMdGeABd71E"
   },
   "outputs": [],
   "source": [
    "with open(cains_fp, 'r', encoding=\"utf-8\") as cains_file:\n",
    "    all_file = cains_file.read()\n",
    "    book_pages = all_file.splitlines()\n",
    "\n",
    "print('pages count: ', len(book_pages))\n",
    "print('1st page: ', book_pages[0])\n",
    "doc_vectors = []\n",
    "\n",
    "for page_text in book_pages:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    page_wv, page_lexicon = get_document_word_vector(page_text)\n",
    "    if not set(page_lexicon).issubset(all_book_lexicon):\n",
    "        print('ERROR! the page lexicon is out of the all text lexicon..')\n",
    "        break\n",
    "    for word, count in page_wv.items():\n",
    "        vec[word] = count / len(all_book_lexicon)\n",
    "    doc_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAKDUcxU0qsr"
   },
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cosine=\", cosine_sim(doc_vectors[0], doc_vectors[0])) #checking our calculations. cosine dist between the same vectors must be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFo30JDHposO"
   },
   "outputs": [],
   "source": [
    "comb = list(combinations(range(100), 2))\n",
    "#print(comb)\n",
    "\n",
    "dict_cos = {}\n",
    "map_cos = np.ones((100,100))\n",
    "for n, item in enumerate(comb):\n",
    "    i = comb[n][0]\n",
    "    j = comb[n][1]\n",
    "    cos_ij = cosine_sim(doc_vectors[i], doc_vectors[j])\n",
    "    dict_cos[\"cosine (%d, %d)\" % (i+1, j+1)] = cos_ij\n",
    "    map_cos[i][j] = cos_ij\n",
    "    map_cos[j][i] = cos_ij\n",
    "\n",
    "desc = sorted(dict_cos.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"pages similarity in decreasing order\\n\")\n",
    "for v, item in enumerate(desc):\n",
    "    print(v+1, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "print(f\"the top {closest} closest pages for each page\\n\")\n",
    "for i in range(100):\n",
    "    print(i + 1, np.argsort(map_cos[i,:])[-N:] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
